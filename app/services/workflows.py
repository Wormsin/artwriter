import json
from services.gemini_api import upload_files, call_llm, upload_small_file, structured_call_llm
from services.preprompts import *
from pathlib import Path
from pydantic import BaseModel, Field
from docx import Document
from docx.shared import Pt
from typing import Dict, List
import uuid
import re
import os

class ChapterStructure(BaseModel):
    chapter_number: int
    chapter_name: str
    chapter_description: str

class ChapterTextScructure(ChapterStructure):
    text: str = ""

class ScriptStructure(BaseModel):
    serie_number: int
    serie_name: str
    content: list[ChapterStructure]

class ScenarioStructure(BaseModel):
    serie_number: int
    serie_name: str
    content: list[ChapterTextScructure]

class ChapterStructureID(ChapterStructure):
    chapter_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
class ScriptStructureID(ScriptStructure):
    serie_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: list[ChapterStructureID]


# --- –£–¢–ò–õ–ò–¢–´ ---
def save_text(content, output_file):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–∞–π–ª."""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)

def save_json(obj, output_file):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç JSON-–æ–±—ä–µ–∫—Ç –≤ —Ñ–∞–π–ª."""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

        

# --- –ü–û–ò–°–ö –î–û–ü –§–ê–ö–¢–û–í ---
def expand_database(topic_path, llm_model_name):
    folder_path = Path(topic_path)
    folder_path_facts = folder_path / "–§–ê–ö–¢–´"
    folder_path_facts.mkdir(parents=True, exist_ok=True)
    folder_path_bd = folder_path / "–ë–î"
    output_file = folder_path_facts / "db_extension.txt"

    file_paths = [str(file.resolve()) for file in folder_path_bd.iterdir() if file.is_file()]
    uploaded_files = upload_files(file_paths)
    prompt = get_stage1_prompt()
    response = call_llm(prompt, files=uploaded_files, 
                        model_name=llm_model_name, 
                        temperature=0.2, 
                        web_search=False)
    save_text(response, output_file)
    print(f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_file}")
    return output_file


# --- –ü–û–ò–°–ö –ù–ï–û–ß–ï–í–ò–î–ù–´–• –°–í–Ø–ó–ï–ô ---
def find_connections(topic_path, llm_model_name):
    output_folder = Path(topic_path) / "–§–ê–ö–¢–´"

    folder = Path(f"{topic_path}/–ë–î")
    file_paths = [str(file.resolve()) for file in folder.iterdir() if file.is_file()]
    uploaded_files = upload_files(file_paths)

    lens = 0
    while True:
        lens+=1
        prompt = get_stage2_prompt_main(lens_num=lens)
        if prompt:
            response = call_llm(prompt, files=uploaded_files, 
                                model_name=llm_model_name, 
                                temperature=1.5,
                                web_search=False)
            #context = response
            output_file = output_folder / f"lens_{lens}_main.txt"
            save_text(response, output_file)
            if lens == 1:
                file_paths = [f"{topic_path}/–§–ê–ö–¢–´/lens_{lens}_main.txt", f"{topic_path}/–§–ê–ö–¢–´/db_extension.txt"]
                uploaded_files = upload_files(file_paths)
            else:
                file_path = f"{topic_path}/–§–ê–ö–¢–´/lens_{lens}_main.txt"
                uploaded_files = upload_small_file(file_path)
        else:
            output_file = output_folder / "db_facts.txt"
            save_text(response, output_file)
            break
    print(f"–ì–∏–ø–æ—Ç–µ–∑—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    return output_file


def find_connections_blind_spots(topic_path, llm_model_name):
    output_folder = Path(topic_path) / "–§–ê–ö–¢–´"

    folder = Path(f"{topic_path}/–ë–î")
    file_paths = [str(file.resolve()) for file in folder.iterdir() if file.is_file()]
    file_paths.append(f"{topic_path}/–§–ê–ö–¢–´/db_extension.txt")
    uploaded_files = upload_files(file_paths)

    lens = 0
    while True:
        lens+=1
        prompt = get_stage2_prompt_blind_spots(lens_num=lens)
        if prompt:
            response = call_llm(prompt, files=uploaded_files, 
                                model_name=llm_model_name, 
                                temperature=1.5,
                                web_search=False)
            
            output_file = output_folder / f"lens_{lens}_blind_spots.txt"
            save_text(response, output_file)
        else:
            return output_file

# --- –ü–†–û–í–ï–†–ö–ê –ì–ò–ü–û–¢–ï–ó ---
def extract_blocks(text, start_tag, end_tag):
    pattern = re.compile(rf"\[{start_tag}\](.*?)\[{end_tag}\]", re.DOTALL)
    return [block.strip() for block in pattern.findall(text)]
def connect_check_hypothese_results(topic_path):
    file_hypothesis = f"{topic_path}/–§–ê–ö–¢–´/db_facts.txt"
    files_checks = [f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked_1.txt", 
                    f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked_2.txt", 
                    f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked_3.txt"]
    output_file = f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked.txt"
    with open(file_hypothesis, "r", encoding="utf-8") as f:
        hypotheses = extract_blocks(f.read(), "–ù–ê–ß–ê–õ–û –ì–ò–ü–û–¢–ï–ó–´", "–ö–û–ù–ï–¶ –ì–ò–ü–û–¢–ï–ó–´")
    all_checks = []
    for path in files_checks:
        with open(path, "r", encoding="utf-8") as f:
            checks = extract_blocks(f.read(), "–ù–ê–ß–ê–õ–û –ü–†–û–í–ï–†–ö–ò", "–ö–û–ù–ï–¶ –ü–†–û–í–ï–†–ö–ò")
            all_checks.append(checks)
    num_hypotheses = len(hypotheses)
    for i, checks in enumerate(all_checks, 1):
        if len(checks) != num_hypotheses:
            print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –≤ —Ñ–∞–π–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ {i} –Ω–∞–π–¥–µ–Ω–æ {len(checks)} –±–ª–æ–∫–æ–≤, "
                f"–∞ –≥–∏–ø–æ—Ç–µ–∑ ‚Äî {num_hypotheses}.")
    merged_blocks = []
    for i in range(num_hypotheses):
        block = "[–ù–ê–ß–ê–õ–û –ü–†–û–í–ï–†–ö–ò –ì–ò–ü–û–¢–ï–ó–´]\n"
        block += hypotheses[i] + "\n\n"
        # –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è —ç—Ç–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã
        for checks in all_checks:
            if i < len(checks):
                block += checks[i] + "\n\n"
        block += "[–ö–û–ù–ï–¶ –ü–†–û–í–ï–†–ö–ò –ì–ò–ü–û–¢–ï–ó–´]\n"
        merged_blocks.append(block)

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(merged_blocks))
    for file_path in files_checks:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"üóëÔ∏è –§–∞–π–ª '{file_path}' —É–¥–∞–ª—ë–Ω.")
        else:
            print(f"‚ö†Ô∏è –§–∞–π–ª '{file_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
    return True

def check_hypotheses(topic_path, llm_model_name):
    file_paths = [f"{topic_path}/–§–ê–ö–¢–´/db_facts.txt"]
    uploaded_files = upload_files(file_paths)
    lens = 0
    while True:
        lens+=1
        prompt = get_stage3_prompt(lens_num=lens)
        if prompt:
            response = call_llm(prompt, files=uploaded_files, 
                                web_search=True, model_name=llm_model_name,
                                temperature=0.2)
            output_file = f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked_{lens}.txt"
            save_text(response, output_file)
            file_paths.append(f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked_{lens}.txt")
            uploaded_files = upload_files(file_paths)
            print(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        else:
            break
    connect_check_hypothese_results(topic_path)
    return output_file



# --- –°–û–ó–î–ê–ù–ò–ï –°–¢–†–£–ö–¢–£–†–´ –°–¶–ï–ù–ê–†–ò–Ø ---
def build_script_structure(topic_path, num_series, llm_model_name):
    output_file_json = f"{topic_path}/–°–¢–†–£–ö–¢–£–†–ê/script_structure.json"
    output_file_txt = f"{topic_path}/–°–¢–†–£–ö–¢–£–†–ê/script_structure.txt"
    folder_path_structure = Path(topic_path)/ "–°–¢–†–£–ö–¢–£–†–ê"
    folder_path_structure.mkdir(parents=True, exist_ok=True)

    folder = Path(f"{topic_path}/–ë–î")
    file_paths = [str(file.resolve()) for file in folder.iterdir() if file.is_file()]
    file_paths.append(f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked.txt")

    uploaded_files = upload_files(file_paths)
    prompt = get_stage4_prompt(num_series)
    response = structured_call_llm(prompt, files=uploaded_files, structure=list[ScriptStructure],
                                    max_output_tokens=65536,
                                    model_name=llm_model_name,
                                    temperature=1)

    scripts: list[ScriptStructure] = response.parsed
    scripts_raw = [script.model_dump() for script in scripts]
    scripts_with_ids: List[ScriptStructureID] = [
    ScriptStructureID.model_validate(script) for script in scripts_raw]
    scripts_ids = [script.model_dump() for script in scripts_with_ids]
    save_json(scripts_ids, output_file_json)
    json_string = json.dumps(
        scripts_ids, 
        indent=2, 
        ensure_ascii=False
    )
    save_text(json_string, output_file_txt)
    print(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_file_json}")
    return output_file_json


# --- –ù–ê–ü–ò–°–ê–ù–ò–ï –¢–ï–ö–°–¢–ê –°–¶–ï–ù–ê–†–ò–Ø---

def scenario_to_docx(output_dir):
    output_path = Path(output_dir)
    input_file = f"{output_dir}/scenario.json"
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    for serie in data:
        doc = Document()
        serie_title = f"{serie['serie_number']}. {serie['serie_name']}"
        doc.add_heading(serie_title, level=0)

        for chapter in serie["content"]:
            # –ù–∞–∑–≤–∞–Ω–∏–µ –≥–ª–∞–≤—ã
            doc.add_heading(f"{chapter['chapter_number']}.   {chapter['chapter_name']}", level=1)

            # –û–ø–∏—Å–∞–Ω–∏–µ –≥–ª–∞–≤—ã (–∫—É—Ä—Å–∏–≤)
            p_desc = doc.add_paragraph()
            run_desc = p_desc.add_run(chapter["chapter_description"])
            run_desc.italic = True
            run_desc.font.size = Pt(11)

            # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç (–æ–±—ã—á–Ω—ã–π)
            p_text = doc.add_paragraph(chapter["text"])
            p_text.style = "Normal"

        file_name = f"–°–µ—Ä–∏—è_{serie['serie_number']}.docx"
        doc.save(output_path / file_name)
        print(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {file_name}")

def get_chapters_per_serie_from_file(file_path: str) -> Dict[int, int]:
    file_path_obj = Path(file_path)
    if not file_path_obj.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {file_path}")
    try:
        # 1. –°—á–∏—Ç—ã–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
        with open(file_path_obj, 'r', encoding='utf-8') as f:
            json_data_string = f.read()
        # 2. –ü–∞—Ä—Å–∏–Ω–≥ JSON-—Å—Ç—Ä–æ–∫–∏ –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
        raw_data = json.loads(json_data_string)
        # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ Pydantic
        # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–∏–π, –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é
        scenario_data: list[ScenarioStructure] = [
            ScenarioStructure.model_validate(item) 
            for item in raw_data
        ]
    except (json.JSONDecodeError, ValueError) as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON –≤ —Ñ–∞–π–ª–µ: {e}")
    # 4. –†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è Dict[int, int]
    chapters_per_serie: Dict[int, int] = {}
    for serie in scenario_data:
        # –ö–ª—é—á: –Ω–æ–º–µ—Ä —Å–µ—Ä–∏–∏; –ó–Ω–∞—á–µ–Ω–∏–µ: –¥–ª–∏–Ω–∞ —Å–ø–∏—Å–∫–∞ content
        chapters_per_serie[serie.serie_number] = len(serie.content)
        
    return chapters_per_serie, scenario_data

def update_json_structure(topic_path):
    file_json = f"{topic_path}/–°–¢–†–£–ö–¢–£–†–ê/script_structure.json"
    file_txt = f"{topic_path}/–°–¢–†–£–ö–¢–£–†–ê/script_structure.txt"
    try:
        with open(file_txt, 'r', encoding='utf-8') as f:
            json_string = f.read()
    except FileNotFoundError:
        print(f"üõë –û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {file_txt}")
        return False
    except Exception as e:
        print(f"üõë –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_txt}: {e}")
        return False
    try:
        data = json.loads(json_string)
    except json.JSONDecodeError as e:
        print(f"üõë –û—à–∏–±–∫–∞: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ '{file_txt}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º JSON.")
        print(f"–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏: {e}")
        return False
    try:
        with open(file_json, 'w', encoding='utf-8') as f:
            json.dump(
                data, 
                f, 
                indent=2, 
                ensure_ascii=False
            )
        print(f"‚úÖ –£—Å–ø–µ—Ö: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ '{file_txt}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ '{file_json}'")
        return True
    except Exception as e:
        print(f"üõë –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ JSON —Ñ–∞–π–ª: {e}")
        return False

def write_script_text(topic_path, llm_model_name, temperature):
    update_json_structure(topic_path=topic_path)
    output_file_json=f"{topic_path}/–°–¶–ï–ù–ê–†–ò–ô/scenario.json"
    folder_path_scenario = Path(topic_path)/ "–°–¶–ï–ù–ê–†–ò–ô"
    folder_path_scenario.mkdir(parents=True, exist_ok=True)
    
    folder_db = Path(topic_path) / "–ë–î"
    file_paths = [str(file.resolve()) for file in folder_db.iterdir() if file.is_file()]
    file_paths.append(f"{topic_path}/–§–ê–ö–¢–´/db_facts_checked.txt")
    file_paths.append(f"{topic_path}/–°–¢–†–£–ö–¢–£–†–ê/script_structure.txt")
    uploaded_files = upload_files(file_paths)
    #uploaded_files = None

    chapters_per_serie, scenario_data = get_chapters_per_serie_from_file(f"{topic_path}/–°–¢–†–£–ö–¢–£–†–ê/script_structure.json")

    previous_chapter_text = ""
    for s in chapters_per_serie:
        for ch in range(1, chapters_per_serie[s]+1):
            prompt = get_stage5_prompt(ser=s, ch=ch, previous_chapter_text=previous_chapter_text)
            response = call_llm(prompt, files=uploaded_files, 
                                model_name=llm_model_name, 
                                temperature=1.3)
            for serie in scenario_data:
                if serie.serie_number == s:
                    target_serie = serie
                    break
            for chapter in target_serie.content:
                if chapter.chapter_number == ch:
                    target_chapter = chapter
                    break
            target_chapter.text = response
            previous_chapter_text = response
    
    try:
        scripts = [sd.model_dump() for sd in scenario_data]
        save_json(scripts, output_file_json)
        print(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_file_json}")
        scenario_to_docx(f"{topic_path}/–°–¶–ï–ù–ê–†–ò–ô")
    except TypeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ NoneType: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ 'response.parsed' –≤–µ—Ä–Ω—É–ª None. –ó–∞–ø—É—Å–∫–∞—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –¥—Ä—É–≥–∞—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")

    return output_file_json
